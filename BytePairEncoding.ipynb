{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SatishWG/BytePairEncoding/blob/main/BytePairEncoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipykernel\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datasets\n",
        "import torch\n",
        "import re\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "RosDK6S_Pz33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CcIbghK7cDya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Hindi Wikipedia dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"zicsx/Wikipedia-Hindi\")"
      ],
      "metadata": {
        "id": "u_OHNr8ke-sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "KW4slY7XgAAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts = \"\"\n",
        "# for example in dataset['train']:\n",
        "#     texts += example['text'] + \"\\n\" # Add a newline to separate texts\n",
        "\n",
        "# print(f\"Total number of characters in 'texts': {len(texts)}\")\n",
        "# # print(texts[:500]) # Display the first 500 characters to verify"
      ],
      "metadata": {
        "id": "mA-YwDBxgvpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read sample data\n",
        "text0 = dataset['train'][0]['text']\n",
        "print(text0)"
      ],
      "metadata": {
        "id": "CFK8QRfjiFPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first 100 rows of the 'text' column\n",
        "texts100 = dataset[\"train\"][\"text\"][:100]\n",
        "\n",
        "# Optional: check the first few entries\n",
        "print(texts100[:5])"
      ],
      "metadata": {
        "id": "couejaqwrlHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the dataset File\n",
        "from datasets import Dataset\n",
        "# Convert the list of texts to a Dataset\n",
        "dataset_10k = Dataset.from_dict({\"text\": texts100})\n",
        "\n",
        "# Save to disk in Hugging Face format\n",
        "dataset_10k.save_to_disk(\"texts100_dataset\")\n",
        "\n",
        "print(\"‚úÖ Dataset saved to 'texts100_dataset/'\")"
      ],
      "metadata": {
        "id": "JfTvEQ-tykIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract only text from the data\n",
        "text = \" \".join(texts100)\n",
        "\n",
        "# Optional: check a snippet\n",
        "print(text[:500])"
      ],
      "metadata": {
        "id": "2EyBLMlo0wRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of text in words: \", len(text))"
      ],
      "metadata": {
        "id": "zBQUXKKvtP3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "js4E696pU6-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "Zs4a__Q3zjYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "4myLqHV0zsly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(text0))\n",
        "print(decode(encode(text0)))"
      ],
      "metadata": {
        "id": "6Idef8FJX9MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "# print(data[:1000]) # the 1000 characters we looked at earier will to the LLM look like this"
      ],
      "metadata": {
        "id": "NUZv14NS0KYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "DpvAk9j00ZEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Regex specific to Hindi"
      ],
      "metadata": {
        "id": "dF-hdzw09kqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct regex pattern (string must be in quotes)\n",
        "\n",
        "devanagari_word = re.compile(\n",
        "    r'(?:[\\u0904-\\u0939\\u0958-\\u0961\\u0966-\\u096F]'\n",
        "    r'(?:[\\u093C\\u094D]?[\\u0904-\\u0939\\u0958-\\u0961])*'\n",
        "    r'[\\u0900-\\u0903\\u093A-\\u094F\\u0951-\\u0957\\u0962-\\u0963]*'\n",
        "    r')+'\n",
        ")\n",
        "\n",
        "dev_text = devanagari_word.findall(text)\n",
        "\n",
        "# print(\"Devanagari words:\", dev_text)\n",
        "print(\"Number of words:\", len(dev_text))"
      ],
      "metadata": {
        "id": "Wt79Uz8mO08X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encdev_text = ' '.join(dev_text)\n",
        "\n",
        "dev_tokens = encdev_text.encode(\"utf-8\") # raw bytes\n",
        "dev_tokens = list(map(int, dev_tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('---')\n",
        "# print(text)\n",
        "print(\"length of text:\", len(encdev_text))\n",
        "print('---')\n",
        "# print(tokens)\n",
        "print(\"length of tokens:\", len(dev_tokens))"
      ],
      "metadata": {
        "id": "Bh3AcafdZTqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
        "# text = \"ÔºµÔΩéÔΩâÔΩÉÔΩèÔΩÑÔΩÖ! üÖ§üÖùüÖòüÖíüÖûüÖìüÖî‚ÄΩ üá∫‚Äåüá≥‚ÄåüáÆ‚Äåüá®‚Äåüá¥‚Äåüá©‚Äåüá™! üòÑ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to ‚Äúsupport Unicode‚Äù in our software (whatever that means‚Äîlike using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don‚Äôt blame programmers for still finding the whole thing mysterious, even 30 years after Unicode‚Äôs inception.\"\n",
        "\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
        "print('---')\n",
        "# print(text)\n",
        "print(\"length of text:\", len(text))\n",
        "print('---')\n",
        "# print(tokens)\n",
        "print(\"length of tokens:\", len(tokens))"
      ],
      "metadata": {
        "id": "7buNo3N6CDi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find the pair of bytes that occur most commonly and then replace them"
      ],
      "metadata": {
        "id": "15iSfhShHbze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "# stats = get_stats(tokens)\n",
        "stats = get_stats(dev_tokens)\n",
        "print(stats)\n",
        "print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
      ],
      "metadata": {
        "id": "ivJdHFbHHMZu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_pair = max(stats, key=stats.get)\n",
        "top_pair"
      ],
      "metadata": {
        "id": "65x7ZBM0J8ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chr(224), chr(164)"
      ],
      "metadata": {
        "id": "M8O3xg7sIwRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge(ids, pair, idx):\n",
        "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    # if we are not at the very last position AND the pair matches, replace it\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "# tokens2 = merge(tokens, top_pair, 256)\n",
        "tokens2 = merge(dev_tokens, top_pair, 256)\n",
        "\n",
        "\n",
        "# print(tokens2)\n",
        "print(\"length:\", len(tokens2), len(dev_tokens))"
      ],
      "metadata": {
        "id": "GZdBZX93JXwD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length:\", len(tokens2), len(tokens))"
      ],
      "metadata": {
        "id": "0NQIKr4VmDsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "length: 2689593 3510382\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "IMTTBkTVo962"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # making the training text longer to have more representative token statistics\n",
        "# Extract the first 1000 rows of the 'text' column\n",
        "text = \" \".join(dataset[\"train\"][\"text\"][:1000])\n",
        "print(\"length of text before regex\",len(text))\n",
        "text = \" \".join(devanagari_word.findall(text))\n",
        "print(\"length of text after regex\",len(text))\n",
        "tokens = text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
      ],
      "metadata": {
        "id": "qH5AHBFjKzVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text), len(tokens2), len(tokens)"
      ],
      "metadata": {
        "id": "D49NsEyUqp9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "# ---\n",
        "vocab_size = 276 # the desired final vocabulary size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) # copy so we don't destroy the original list\n",
        "\n",
        "merges = {} # (int, int) -> int\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merging {pair} into a new token {idx}\")\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx"
      ],
      "metadata": {
        "id": "0RJF97WmL-Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ],
      "metadata": {
        "id": "BOw4ulpFM0l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "# ---\n",
        "vocab_size = 5000 # the desired final vocabulary size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) # copy so we don't destroy the original list\n",
        "\n",
        "merges = {} # (int, int) -> int\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merging {pair} into a new token {idx}\")\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx\n",
        "\n",
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ],
      "metadata": {
        "id": "2vmogX5kM3x-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoding\n",
        "\n",
        "Given a sequence of integers in the range [0, vocab_size], what is the text?\n"
      ],
      "metadata": {
        "id": "GOv7J5O5QDNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merges.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]\n",
        "\n",
        "def decode(ids):\n",
        "  # given ids (list of integers), return Python string\n",
        "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "  return text\n",
        "\n",
        "print(decode([128, 255, 233]))"
      ],
      "metadata": {
        "id": "aVBn0l-qM7gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding\n",
        "\n",
        "The other way around: Given a string, what are the tokens?\n"
      ],
      "metadata": {
        "id": "k-mlB1AdQ34_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# merges\n",
        "len(merges)"
      ],
      "metadata": {
        "id": "fM1nBKb_Qdv1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "  # given a string, return list of integers (the tokens)\n",
        "  tokens = list(text.encode(\"utf-8\"))\n",
        "  while len(tokens) >= 2:\n",
        "    stats = get_stats(tokens)\n",
        "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
        "    if pair not in merges:\n",
        "      break # nothing else can be merged\n",
        "    idx = merges[pair]\n",
        "    tokens = merge(tokens, pair, idx)\n",
        "  return tokens\n",
        "\n",
        "print(encode(\"\"))"
      ],
      "metadata": {
        "id": "mtbHxWrKRAT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(text0)))"
      ],
      "metadata": {
        "id": "6Va4rDHPRWbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = decode(encode(text0))\n",
        "print(text2 == text0)"
      ],
      "metadata": {
        "id": "N_8bbbpdRX62"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}